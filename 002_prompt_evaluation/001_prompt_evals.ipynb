{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ce5d99",
   "metadata": {},
   "source": [
    "# Prompt Evaluation\n",
    "\n",
    "**Prompt Evaluation** is the process of testing the efficacy of a prompt. The goal of prompt evaluation is to get an objective metric that tells us if our prompt is effective (or not). While there are manual methods for prompt evaluation, often, the best approach is to define an evaluation workflow that automatically tests and scores prompts, allowing developers to easily see how changes to their prompts effect performance.\n",
    "\n",
    "A typical prompt evaluation workflow might look like this:\n",
    "- Step 1: Draft a prompt\n",
    "- Step 2: Create an evaluation dataset\n",
    "- Step 3: Test and score the prompt (using the evaluation dataset)\n",
    "- Step 4: Iterate\n",
    "\n",
    "Scoring is generally achieved by scoring each test case in the evaluation set individually and then averaging all the individual test scores to derive and aggregate score.\n",
    "\n",
    "This notebook offers an implementation of a rudimentary prompt evaluation workflow.\n",
    "\n",
    "**Note:** There are open source (e.g., [DeepEval](https://github.com/confident-ai/deepeval) and [Promptfoo](https://github.com/promptfoo/promptfoo)) and paid packages that can help you develop a prompt evaluation workflow. We are writing our own below to:\n",
    "- Demonstrate that you don't *require* full featured tools to perform prompt evaluation\n",
    "- Build competency with the concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e9a509",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3f07feb2",
   "metadata": {},
   "source": [
    "## Set up the Environment and Create Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5437be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables and create client\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from anthropic import Anthropic\n",
    "import json\n",
    "\n",
    "load_dotenv()   # Load environment variables from .env file\n",
    "\n",
    "# Create an API client\n",
    "client = Anthropic()\n",
    "model = \"claude-haiku-4-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0d8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions for managing message history and chatting with Claude\n",
    "\n",
    "def add_user_message(messages : list[str], content : str):\n",
    "    \"\"\"Append a user message to a message (aka conversation) history.\n",
    "\n",
    "    Args:\n",
    "        messages (list[str]): The message history.\n",
    "        content (str): A user message to append to the message history.\n",
    "    \"\"\"\n",
    "    user_message = { \"role\": \"user\", \"content\": content }\n",
    "    messages.append(user_message)\n",
    "\n",
    "def add_assistant_message(messages : list[str], content : str):\n",
    "    \"\"\"Append an assistant message to a message (aka conversation) history.\n",
    "\n",
    "    Args:\n",
    "        messages (list[str]): The message history.\n",
    "        content (str): An assistant message to append to the message history.\n",
    "    \"\"\"\n",
    "    assistant_message = { \"role\": \"assistant\", \"content\": content }\n",
    "    messages.append(assistant_message)\n",
    "\n",
    "def chat(messages : list[str],\n",
    "         system : str = None,\n",
    "         temperature : float =1.0,\n",
    "         stop_sequences: list[str] = []) -> str:\n",
    "    \"\"\"Chat with Claude.\n",
    "\n",
    "    Args:\n",
    "        messages (list[str]): The message (aka conversation) history.\n",
    "        system (str): The system prompt. Default to None\n",
    "        temperature (float, optional): Temperature for response generation. Defaults to 1.0.\n",
    "        stop_sequences (list[str], optional): Stop sequences for Claude's response. Defaults to [].\n",
    "\n",
    "    Returns:\n",
    "        str: Claude's text response.\n",
    "    \"\"\"\n",
    "    response = client.messages.create(\n",
    "        model=model,\n",
    "        max_tokens=1000,\n",
    "        messages=messages,\n",
    "        system=\"Provide concise answers.\",\n",
    "        stop_sequences=stop_sequences\n",
    "    )\n",
    "    return response.content[0].text\n",
    "\n",
    "def chat(messages, system=None, temperature=1.0, stop_sequences=[]):\n",
    "    params = {\n",
    "        \"model\": model,\n",
    "        \"max_tokens\": 1000,\n",
    "        \"messages\": messages,\n",
    "        \"temperature\": temperature,\n",
    "        \"stop_sequences\": stop_sequences,\n",
    "    }\n",
    "\n",
    "    # Add a system parameter to the dictionary only if provided\n",
    "    if system:\n",
    "        params[\"system\"] = system\n",
    "\n",
    "    response = client.messages.create(**params)\n",
    "\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35258787",
   "metadata": {},
   "source": [
    "## Evaluation Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e191fe",
   "metadata": {},
   "source": [
    "### Step 1: Draft a Prompt\n",
    "\n",
    "Our prompt's goal is to assist users in writing three specific types of output for AWS use cases. The output could be:\n",
    "\n",
    "- Python code\n",
    "- A JSON configuration files\n",
    "- A regular expressions\n",
    "\n",
    "The key requirement is that when a user requests help with a task, we return clean output in one of these formats without any extra explanations, headers, or footers.\n",
    "\n",
    "Version 1 of our prompt, which is *super* naive, is as follows:\n",
    "\n",
    "```python\n",
    "prompt_v1 = \"\"\"\n",
    "    Please provide a solution to the following task:\n",
    "    {{task}}\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b42715",
   "metadata": {},
   "source": [
    "### Step 2: Create Evaluation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5ea049",
   "metadata": {},
   "source": [
    "First, define a function for creating our evaluation dataset.\n",
    "\n",
    "In this example, the evaluation dataset will:\n",
    "- Be generated by Claude\n",
    "- Contain only three entries\n",
    "\n",
    "In \"real life\" the evaluation dataset would be much larger and would likely contain a combination of human generated and machine generated entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1e788701",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for generating an evaluation dataset\n",
    "\n",
    "import json     # For validating JSON data\n",
    "\n",
    "def generate_dataset():\n",
    "\n",
    "    prompt = \"\"\"\n",
    "        Generate a evaluation dataset for a prompt evaluation. The dataset will be used to evaluate\n",
    "        prompts that generate Python, JSON, or Regex specifically for AWS-related tasks. Generate an\n",
    "        array of JSON objects, each representing a task that requires Python, JSON, or a Regex to\n",
    "        complete.\n",
    "\n",
    "        Example output:\n",
    "        ```json\n",
    "        [\n",
    "            {\n",
    "                \"task\": \"Description of task\",\n",
    "                \"format\": \"python\" or \"json\" or \"regex\"\n",
    "            },\n",
    "            ...additional\n",
    "        ]\n",
    "        ```\n",
    "\n",
    "        * Focus on tasks that can be solved by writing a single Python function, a single JSON object,\n",
    "        or a regular expression.\n",
    "        * Focus on tasks that do not require writing much code\n",
    "\n",
    "        Please generate 3 objects.\n",
    "        \"\"\"\n",
    "    \n",
    "    messages = []\n",
    "    add_user_message(messages, prompt)\n",
    "    add_assistant_message(messages, \"```json\")\n",
    "    response = chat(messages, stop_sequences=[\"```\"])\n",
    "    return json.loads(response)     # the json.loads will raise an error if response is not valid JSON\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc48e3b6",
   "metadata": {},
   "source": [
    "Next, create the evaluation dataset and write it to a file named `dataset.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56bb803a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the evaluation dataset and save it to a file\n",
    "\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# Write the evaluation dataset to a file\n",
    "with open(\"dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c33fe75",
   "metadata": {},
   "source": [
    "### Step 3: Test and Score the Prompt\n",
    "\n",
    "In this step, we take each task in the evaluation dataset, merge it with our prompt to create a test case, feed the test case into Claude, and then score the results.\n",
    "\n",
    "The most complex operation in this step is scoring. Generally speaking, there are three types of scorers:\n",
    "\n",
    "- **Code Scorers** are programmatic scorers written by humans. These types of scorers can do things like evaluate the length of a response, verify key words, syntax validation, etc.\n",
    "- **Model Scorers** are typically language models that evaluate the quality, completeness, helpfulness, safety, etc. of the output\n",
    "- **Human Scorers**, which often provide the best quality scoring, but are generally slower and more expensive than the above\n",
    "\n",
    "In the example below, we will use both code and model scorers.\n",
    "\n",
    "Before we implement scorers, however, we will need to define evaluation criteria. For our example, we will evaluate output based on:\n",
    "\n",
    "- **Format/Syntax** - Responses should be python code, JSON, or regex *only*. Responses should contain valid code syntax.\n",
    "- **Accuracy** - Responses should accurately, directly, and clearly address the task\n",
    "\n",
    "We will be using:\n",
    "\n",
    "- Code scorers for format and syntax\n",
    "- A model scorer for accuracy\n",
    "\n",
    "We start by defining functions that will accept various prompts, create the test cases, and score the output.\n",
    "\n",
    "**Note:** The `run_test_case` and `score_the_accuracy` methods below uses the *prefilled assistant messages* and *stop sequences* techniques introduced in [005_controlling_output.ipynb](001_accessing_claude_with_the_api/005_controlling_output.ipynb) and [006_structured_data.ipynb](001_accessing_claude_with_the_api/006_structured_data.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "29a6470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for evaluating a prompt using the evaluation dataset\n",
    "\n",
    "from statistics import mean \n",
    "import re                       # For validating regex patterns\n",
    "import ast                      # For validating Python code\n",
    "\n",
    "# Merge the prompt with a specific test case\n",
    "def merge_task_with_prompt(task, prompt):\n",
    "    return prompt.replace(\"{{task}}\", task)\n",
    "\n",
    "# Run a single test case\n",
    "def run_test_case(test_case):\n",
    "    messages = []\n",
    "    add_user_message(messages, test_case)\n",
    "    add_assistant_message(messages, \"```code\")\n",
    "    response = chat(messages, stop_sequences=[\"```\"])\n",
    "    return response\n",
    "\n",
    "# Score the format/syntax of the response using a code scorer\n",
    "def score_the_syntax(solution, expected_format):\n",
    "    if expected_format == \"json\":\n",
    "        try:\n",
    "            json.loads(solution.strip())\n",
    "            return 10\n",
    "        except json.JSONDecodeError:\n",
    "            return 0\n",
    "    elif expected_format == \"python\":\n",
    "        try:\n",
    "            ast.parse(solution.strip())\n",
    "            return 10\n",
    "        except SyntaxError:\n",
    "            return 0\n",
    "    else:\n",
    "        try:\n",
    "            re.compile(solution.strip())\n",
    "            return 10\n",
    "        except re.error:\n",
    "            return 0\n",
    "\n",
    "# Score the accuracy of the response using a model scorer\n",
    "def score_the_accuracy(task, solution):\n",
    "\n",
    "    # Create the evaluation prompt\n",
    "    # Note: If you only ask Claude to score the accuracy, it often tends to give a score of 6. You must ask\n",
    "    # for strengths, weaknesses, and reasoning to get a more balanced evaluation.\n",
    "    eval_prompt = \"\"\"\n",
    "        You are an expert AWS code reviewer. Your task is to evaluate the following AI-generated solution.\n",
    "\n",
    "        Original Task:\n",
    "        <task>\n",
    "        {{task}}\n",
    "        </task>\n",
    "\n",
    "        Solution to Evaluate:\n",
    "        <solution>\n",
    "        {{solution}}\n",
    "        </solution>\n",
    "\n",
    "        Output Format\n",
    "        Provide your evaluation as a structured JSON object with the following fields, in this specific order:\n",
    "        - \"strengths\": An array of 1-3 key strengths\n",
    "        - \"weaknesses\": An array of 1-3 key areas for improvement\n",
    "        - \"reasoning\": A concise explanation of your overall assessment\n",
    "        - \"score\": A number between 1-10\n",
    "\n",
    "        Respond with JSON. Keep your response concise and direct.\n",
    "        Example response shape:\n",
    "        {{\n",
    "            \"strengths\": string[],\n",
    "            \"weaknesses\": string[],\n",
    "            \"reasoning\": string,\n",
    "            \"score\": number\n",
    "        }}\n",
    "    \"\"\"    \n",
    "    eval_prompt = eval_prompt.replace(\"{{task}}\", task)\n",
    "    eval_prompt = eval_prompt.replace(\"{{solution}}\", solution)\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    # Prepare the messages for the using a prefilled assistant message and a stop sequence\n",
    "    add_user_message(messages, eval_prompt)\n",
    "    add_assistant_message(messages, \"Your evaluation in JSON format is as follows:\\n```json\")\n",
    "    response = chat(messages, stop_sequences=[\"```\"])\n",
    "\n",
    "    # Convert the JSON into a dict for easy access to the data (in Python).\n",
    "    # Note: The json.loads will raise an error if response is not valid JSON\n",
    "    try:\n",
    "        response = json.loads(response)\n",
    "    except json.JSONDecodeError:\n",
    "        response = {\n",
    "            \"strengths\": [],\n",
    "            \"weaknesses\": [],\n",
    "            \"reasoning\": \"Response was not valid JSON.\",\n",
    "            \"score\": 0\n",
    "        }\n",
    "\n",
    "    return response\n",
    "\n",
    "# Evaluate the prompt on the entire dataset\n",
    "def evaluate_prompt(prompt, dataset):\n",
    "    results = []\n",
    "    scores = []\n",
    "    for record in dataset:\n",
    "\n",
    "        # Run the test case\n",
    "        test_case = merge_task_with_prompt(record[\"task\"], prompt)\n",
    "        response = run_test_case(test_case)\n",
    "\n",
    "        # Score the response\n",
    "        syntax_score = score_the_syntax(response, record[\"format\"])\n",
    "        accuracy_object = score_the_accuracy(record[\"task\"], response)\n",
    "        accuracy_score = accuracy_object[\"score\"]\n",
    "        accuracy_reasoning = accuracy_object[\"reasoning\"]\n",
    "\n",
    "        # Get the average score\n",
    "        individual_score = mean([syntax_score, accuracy_score])\n",
    "        \n",
    "        # Store the score for overall averaging later\n",
    "        scores.append(individual_score)\n",
    "\n",
    "        # Store the results of the each evaluation\n",
    "        results.append({\n",
    "            \"task\": record[\"task\"],\n",
    "            \"output\": response,\n",
    "            \"syntax_score\": syntax_score,\n",
    "            \"accuracy_score\": accuracy_score,\n",
    "            \"accuracy_reasoning\": accuracy_reasoning,\n",
    "            \"score\": individual_score\n",
    "        })\n",
    "\n",
    "    # Calculate and print the overall average score\n",
    "    overall_average_score = mean(scores)\n",
    "    print(f\"Average score (across all test cases) for this prompt: {overall_average_score}\")\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f3d57d",
   "metadata": {},
   "source": [
    "### Step 4: Iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d5437a",
   "metadata": {},
   "source": [
    "Here is the first prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3f6cf91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score (across all test cases) for this prompt: 4.833333333333333\n",
      "[\n",
      "    {\n",
      "        \"task\": \"Parse an AWS S3 bucket name from a full S3 URI (e.g., 's3://my-bucket/path/to/object') and extract only the bucket name\",\n",
      "        \"output\": \"\\n# Solution 1: Using string methods (Simple and straightforward)\\ndef extract_bucket_name_simple(s3_uri):\\n    \\\"\\\"\\\"\\n    Extract bucket name from S3 URI using string methods.\\n    \\n    Args:\\n        s3_uri (str): Full S3 URI (e.g., 's3://my-bucket/path/to/object')\\n    \\n    Returns:\\n        str: The bucket name\\n    \\\"\\\"\\\"\\n    # Remove the 's3://' prefix and split by '/'\\n    return s3_uri.replace('s3://', '').split('/')[0]\\n\\n\\n# Solution 2: Using urllib.parse (More robust)\\nfrom urllib.parse import urlparse\\n\\ndef extract_bucket_name_urlparse(s3_uri):\\n    \\\"\\\"\\\"\\n    Extract bucket name from S3 URI using urllib.parse.\\n    \\n    Args:\\n        s3_uri (str): Full S3 URI (e.g., 's3://my-bucket/path/to/object')\\n    \\n    Returns:\\n        str: The bucket name\\n    \\\"\\\"\\\"\\n    parsed = urlparse(s3_uri)\\n    return parsed.netloc\\n\\n\\n# Solution 3: Using regex (Most flexible)\\nimport re\\n\\ndef extract_bucket_name_regex(s3_uri):\\n    \\\"\\\"\\\"\\n    Extract bucket name from S3 URI using regex.\\n    \\n    Args:\\n        s3_uri (str): Full S3 URI (e.g., 's3://my-bucket/path/to/object')\\n    \\n    Returns:\\n        str: The bucket name or None if not found\\n    \\\"\\\"\\\"\\n    match = re.match(r's3://([^/]+)', s3_uri)\\n    return match.group(1) if match else None\\n\\n\\n# Solution 4: Using boto3 (AWS SDK approach)\\nfrom botocore.util import parse_s3_url\\n\\ndef extract_bucket_name_boto3(s3_uri):\\n    \\\"\\\"\\\"\\n    Extract bucket name from S3 URI using boto3's utility function.\\n    \\n    Args:\\n        s3_uri (str): Full S3 URI (e.g., 's3://my-bucket/path/to/object')\\n    \\n    Returns:\\n        str: The bucket name\\n    \\\"\\\"\\\"\\n    parsed = parse_s3_url(s3_uri)\\n    return parsed['bucket']\\n\\n\\n# Test cases\\nif __name__ == \\\"__main__\\\":\\n    test_uris = [\\n        's3://my-bucket/path/to/object',\\n        's3://my-bucket/',\\n        's3://my-bucket',\\n        's3://bucket-with-dashes/deep/nested/path/file.txt',\\n        's3://bucket123/file.txt'\\n    ]\\n    \\n    print(\\\"Testing all solutions:\\\\n\\\")\\n    \\n    for uri in test_uris:\\n        print(f\\\"Input: {uri}\\\")\\n        print(f\\\"  Simple:    {extract_bucket_name_simple(uri)}\\\")\\n        print(f\\\"  URLParse:  {extract_bucket_name_urlparse(uri)}\\\")\\n        print(f\\\"  Regex:     {extract_bucket_name_regex(uri)}\\\")\\n        try:\\n            print(f\\\"  Boto3:     {extract_bucket_name_boto3(uri)}\\\")\\n        except Exception as e:\\n            print(f\\\"  Boto3:     Error - {e}\\\")\\n        print()\\n\",\n",
      "        \"syntax_score\": 0,\n",
      "        \"accuracy_score\": 6,\n",
      "        \"accuracy_reasoning\": \"The solution demonstrates good AWS knowledge by including boto3 as the recommended approach and provides helpful alternatives. However, it has critical gaps: incomplete testing (the test cases don't actually validate failures), misleading descriptions about robustness, and missing input validation. For production AWS code, this needs error handling and clear documentation of failure modes. The boto3 solution is best but should be presented as the primary recommendation with others as alternatives.\",\n",
      "        \"score\": 3\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Create a Python function that takes an AWS IAM policy document as a JSON string and returns a list of all the Resource ARNs referenced in the policy\",\n",
      "        \"output\": \"\\nimport json\\nfrom typing import List\\n\\ndef extract_resource_arns(policy_document: str) -> List[str]:\\n    \\\"\\\"\\\"\\n    Extract all Resource ARNs from an AWS IAM policy document.\\n    \\n    Args:\\n        policy_document (str): AWS IAM policy document as a JSON string\\n        \\n    Returns:\\n        List[str]: A list of all Resource ARNs found in the policy\\n        \\n    Raises:\\n        json.JSONDecodeError: If the policy document is not valid JSON\\n        ValueError: If the policy document is not a valid IAM policy structure\\n        \\n    Example:\\n        >>> policy = '''\\n        ... {\\n        ...     \\\"Version\\\": \\\"2012-10-17\\\",\\n        ...     \\\"Statement\\\": [\\n        ...         {\\n        ...             \\\"Effect\\\": \\\"Allow\\\",\\n        ...             \\\"Action\\\": \\\"s3:GetObject\\\",\\n        ...             \\\"Resource\\\": \\\"arn:aws:s3:::my-bucket/*\\\"\\n        ...         }\\n        ...     ]\\n        ... }\\n        ... '''\\n        >>> extract_resource_arns(policy)\\n        ['arn:aws:s3:::my-bucket/*']\\n    \\\"\\\"\\\"\\n    try:\\n        policy = json.loads(policy_document)\\n    except json.JSONDecodeError as e:\\n        raise json.JSONDecodeError(f\\\"Invalid JSON in policy document: {e.msg}\\\", e.doc, e.pos)\\n    \\n    resource_arns = []\\n    \\n    # Validate that the policy has a Statement\\n    if not isinstance(policy, dict):\\n        raise ValueError(\\\"Policy document must be a JSON object\\\")\\n    \\n    statements = policy.get(\\\"Statement\\\", [])\\n    \\n    if not isinstance(statements, list):\\n        raise ValueError(\\\"Statement must be a list\\\")\\n    \\n    # Iterate through each statement\\n    for statement in statements:\\n        if not isinstance(statement, dict):\\n            continue\\n            \\n        # Extract Resource field (can be string or list)\\n        resources = statement.get(\\\"Resource\\\")\\n        \\n        if resources is None:\\n            continue\\n        \\n        # Handle both single string and list of strings\\n        if isinstance(resources, str):\\n            resource_arns.append(resources)\\n        elif isinstance(resources, list):\\n            for resource in resources:\\n                if isinstance(resource, str):\\n                    resource_arns.append(resource)\\n    \\n    return resource_arns\\n\\n\\n# Test cases\\nif __name__ == \\\"__main__\\\":\\n    # Test 1: Single resource as string\\n    policy1 = '''\\n    {\\n        \\\"Version\\\": \\\"2012-10-17\\\",\\n        \\\"Statement\\\": [\\n            {\\n                \\\"Effect\\\": \\\"Allow\\\",\\n                \\\"Action\\\": \\\"s3:GetObject\\\",\\n                \\\"Resource\\\": \\\"arn:aws:s3:::my-bucket/*\\\"\\n            }\\n        ]\\n    }\\n    '''\\n    print(\\\"Test 1 - Single resource:\\\")\\n    print(extract_resource_arns(policy1))\\n    # Output: ['arn:aws:s3:::my-bucket/*']\\n    \\n    # Test 2: Multiple resources as list\\n    policy2 = '''\\n    {\\n        \\\"Version\\\": \\\"2012-10-17\\\",\\n        \\\"Statement\\\": [\\n            {\\n                \\\"Effect\\\": \\\"Allow\\\",\\n                \\\"Action\\\": \\\"s3:*\\\",\\n                \\\"Resource\\\": [\\n                    \\\"arn:aws:s3:::bucket1/*\\\",\\n                    \\\"arn:aws:s3:::bucket2/*\\\"\\n                ]\\n            }\\n        ]\\n    }\\n    '''\\n    print(\\\"\\\\nTest 2 - Multiple resources in list:\\\")\\n    print(extract_resource_arns(policy2))\\n    # Output: ['arn:aws:s3:::bucket1/*', 'arn:aws:s3:::bucket2/*']\\n    \\n    # Test 3: Multiple statements with mixed resource types\\n    policy3 = '''\\n    {\\n        \\\"Version\\\": \\\"2012-10-17\\\",\\n        \\\"Statement\\\": [\\n            {\\n                \\\"Effect\\\": \\\"Allow\\\",\\n                \\\"Action\\\": \\\"s3:GetObject\\\",\\n                \\\"Resource\\\": \\\"arn:aws:s3:::my-bucket/*\\\"\\n            },\\n            {\\n                \\\"Effect\\\": \\\"Allow\\\",\\n                \\\"Action\\\": \\\"ec2:\",\n",
      "        \"syntax_score\": 0,\n",
      "        \"accuracy_score\": 6,\n",
      "        \"accuracy_reasoning\": \"The solution demonstrates solid AWS IAM knowledge and good Python practices with proper typing and error handling. The core logic correctly extracts Resource ARNs from statement objects in both string and list formats. However, the code is incomplete due to truncation in the test cases, which prevents verification of full functionality. Additionally, the solution doesn't account for NotResource fields that can appear in IAM policies. The function works well for the primary use case but lacks comprehensive handling of all IAM policy variations.\",\n",
      "        \"score\": 3\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Generate a JSON object representing an AWS Lambda function configuration with environment variables, memory allocation set to 256 MB, and a timeout of 30 seconds\",\n",
      "        \"output\": \"\\n{\\n  \\\"FunctionName\\\": \\\"my-function\\\",\\n  \\\"Runtime\\\": \\\"python3.11\\\",\\n  \\\"Role\\\": \\\"arn:aws:iam::123456789012:role/lambda-role\\\",\\n  \\\"Handler\\\": \\\"index.handler\\\",\\n  \\\"Code\\\": {\\n    \\\"S3Bucket\\\": \\\"my-bucket\\\",\\n    \\\"S3Key\\\": \\\"function.zip\\\"\\n  },\\n  \\\"Description\\\": \\\"My Lambda function\\\",\\n  \\\"Timeout\\\": 30,\\n  \\\"MemorySize\\\": 256,\\n  \\\"Environment\\\": {\\n    \\\"Variables\\\": {\\n      \\\"DB_HOST\\\": \\\"database.example.com\\\",\\n      \\\"DB_PORT\\\": \\\"5432\\\",\\n      \\\"DB_NAME\\\": \\\"mydb\\\",\\n      \\\"API_KEY\\\": \\\"your-api-key-here\\\",\\n      \\\"LOG_LEVEL\\\": \\\"INFO\\\",\\n      \\\"ENVIRONMENT\\\": \\\"production\\\"\\n    }\\n  },\\n  \\\"VpcConfig\\\": {\\n    \\\"SubnetIds\\\": [\\n      \\\"subnet-12345678\\\"\\n    ],\\n    \\\"SecurityGroupIds\\\": [\\n      \\\"sg-12345678\\\"\\n    ]\\n  },\\n  \\\"Tags\\\": {\\n    \\\"Environment\\\": \\\"Production\\\",\\n    \\\"Project\\\": \\\"MyProject\\\",\\n    \\\"Owner\\\": \\\"DevTeam\\\"\\n  }\\n}\\n\",\n",
      "        \"syntax_score\": 10,\n",
      "        \"accuracy_score\": 7,\n",
      "        \"accuracy_reasoning\": \"The solution fully satisfies the core requirements and demonstrates a well-structured Lambda configuration. However, it contains a critical security vulnerability by storing sensitive credentials as plain-text environment variables. While this is a common mistake in examples, production configurations should always use AWS Secrets Manager or Systems Manager Parameter Store for sensitive data. The use of placeholder values also indicates the configuration isn't ready for actual deployment.\",\n",
      "        \"score\": 8.5\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt_v1 = \"\"\"\n",
    "    Please provide a solution to the following task: {{task}}\n",
    "\"\"\"\n",
    "\n",
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = evaluate_prompt(prompt_v1, dataset)\n",
    "print(json.dumps(results, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4babb68",
   "metadata": {},
   "source": [
    "Here is the (trivially different) second prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3982f442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average score (across all test cases) for this prompt: 6.5\n",
      "[\n",
      "    {\n",
      "        \"task\": \"Parse an AWS S3 bucket name from a full S3 URI (e.g., 's3://my-bucket/path/to/object') and extract only the bucket name\",\n",
      "        \"output\": \"\\nimport re\\n\\ndef extract_bucket_name(s3_uri):\\n    match = re.match(r's3://([a-z0-9.-]+)/', s3_uri)\\n    if match:\\n        return match.group(1)\\n    return None\\n\",\n",
      "        \"syntax_score\": 10,\n",
      "        \"accuracy_score\": 5,\n",
      "        \"accuracy_reasoning\": \"The solution works for the most common case (URIs with paths) but has significant limitations. The trailing slash requirement is a critical flaw since 's3://my-bucket' is a valid S3 URI. The regex pattern also doesn't validate AWS bucket naming constraints, which could allow invalid bucket names through. The implementation would benefit from more robust pattern matching and input validation.\",\n",
      "        \"score\": 7.5\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Create a Python function that takes an AWS IAM policy document as a JSON string and returns a list of all the Resource ARNs referenced in the policy\",\n",
      "        \"output\": \"\\nimport json\\nimport re\\n\\ndef extract_resource_arns(policy_json_string: str) -> list[str]:\\n    \\\"\\\"\\\"\\n    Extract all Resource ARNs from an AWS IAM policy document.\\n    \\n    Args:\\n        policy_json_string: AWS IAM policy document as a JSON string\\n        \\n    Returns:\\n        List of all Resource ARNs found in the policy\\n    \\\"\\\"\\\"\\n    try:\\n        policy = json.loads(policy_json_string)\\n    except json.JSONDecodeError:\\n        return []\\n    \\n    resources = []\\n    \\n    if isinstance(policy, dict) and \\\"Statement\\\" in policy:\\n        statements = policy[\\\"Statement\\\"]\\n        if not isinstance(statements, list):\\n            statements = [statements]\\n        \\n        for statement in statements:\\n            if isinstance(statement, dict) and \\\"Resource\\\" in statement:\\n                resource = statement[\\\"Resource\\\"]\\n                if isinstance(resource, str):\\n                    resources.append(resource)\\n                elif isinstance(resource, list):\\n                    resources.extend(resource)\\n    \\n    return resources\\n\",\n",
      "        \"syntax_score\": 10,\n",
      "        \"accuracy_score\": 7,\n",
      "        \"accuracy_reasoning\": \"The solution correctly addresses the core requirement and handles the most common IAM policy structure. However, it has limited scope\\u2014it only extracts from Resource fields and ignores NotResource, and it provides no feedback on parsing failures. The implementation is practical for simple policies but lacks comprehensiveness for complex real-world AWS IAM policies that may use alternative resource specifications.\",\n",
      "        \"score\": 8.5\n",
      "    },\n",
      "    {\n",
      "        \"task\": \"Generate a JSON object representing an AWS Lambda function configuration with environment variables, memory allocation set to 256 MB, and a timeout of 30 seconds\",\n",
      "        \"output\": \"\\nimport json\\n\\nlambda_config = {\\n    \\\"FunctionName\\\": \\\"my-lambda-function\\\",\\n    \\\"Runtime\\\": \\\"python3.11\\\",\\n    \\\"Role\\\": \\\"arn:aws:iam::123456789012:role/lambda-execution-role\\\",\\n    \\\"Handler\\\": \\\"index.handler\\\",\\n    \\\"MemorySize\\\": 256,\\n    \\\"Timeout\\\": 30,\\n    \\\"Environment\\\": {\\n        \\\"Variables\\\": {\\n            \\\"ENV\\\": \\\"production\\\",\\n            \\\"DEBUG\\\": \\\"false\\\",\\n            \\\"API_KEY\\\": \\\"your-api-key-here\\\"\\n        }\\n    },\\n    \\\"Description\\\": \\\"AWS Lambda function configuration\\\"\\n}\\n\\nprint(json.dumps(lambda_config, indent=2))\\n\",\n",
      "        \"syntax_score\": 0,\n",
      "        \"accuracy_score\": 7,\n",
      "        \"accuracy_reasoning\": \"The solution directly addresses all explicit requirements with correct syntax and structure. However, it introduces a security vulnerability by embedding credentials in environment variables, which is a significant AWS best practice violation. The code is otherwise well-formatted and functional, making it suitable for demonstration but not production use without security improvements.\",\n",
      "        \"score\": 3.5\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "prompt_v2 = \"\"\"\n",
    "    Please provide a solution to the following task: {{task}}\n",
    "    Respond only with Python, JSON, or a plain Regex.\n",
    "    Do not add any comments or commentary or explanation.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"dataset.json\", \"r\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "results = evaluate_prompt(prompt_v2, dataset)\n",
    "print(json.dumps(results, indent=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cad7934",
   "metadata": {},
   "source": [
    "Per the results above (which could vary run to run), you may see improvement in the average score (across all test cases) between version 1 and version 2 of the prompt. If you want to see more dramatic improvement, you could (as [this](https://anthropic.skilljar.com/claude-with-the-anthropic-api/287738) video suggests):\n",
    "\n",
    "- Update the dataset generation prompt to ask for solution criteria\n",
    "- Update the prompt in the `score_the_accuracy` function to include that solution criteria"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
